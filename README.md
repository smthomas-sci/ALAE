# ALAE Implentations for Pytorch and Tensorflow 1&2 on MNIST Digits

The following are the result from the MLP archictures described in [Adversarial Latent Autoencoders](https://arxiv.org/abs/2004.04467) by Pidhorskyi *et al.* (2020).

Training match the hyper-parameters described in the paper, with gamma=0.1 as left unspecified.

### PyTorch
![PyTorch Gif](./PT_MNIST_100_epochs.gif)

![PyTorch Curves](./PT_MNIST_Training_Curve_100_epochs.png)

### Tensorflow v1 
![TF1 Result](./TF_MNIST_100_epochs.png)

![TF1 Curves](./TF_MNIST_traing_curve_100_epochs.png)

### Tensorflow v2 
![TF2 Result](./TF2_MNIST_100_epochs.png.png)

![TF2 Curves](./TF2_MNIST_traing_curve_100_epochs.png)
