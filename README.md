# ALAE Implentations for Tensorflow 1 and Pytorch on MNIST Digits
<hr>

The following are the result from the MLP archictures described in [Adversarial Latent Autoencoders](https://arxiv.org/abs/2004.04467) by Pidhorskyi *et al.* (2020).

Training match the hyper-parameters described in the paper, with gamma=0.1 as left unspecified.

### PyTorch
![PyTorch Gif](./PT_MNIST_100_epochs.gif)

![PyTorch Curves](./PT_MNIST_Training_Curve_100_epochs.png)

### Tensorflow v1 
![TF1 Result](./TF_MNIST_100_epochs.png)

![TF1 Curves](./TF_MNIST_traing_curve_100_epochs.png)
