{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from tensorflow_addons.layers import InstanceNormalization\n",
    "from tensorflow.keras.layers import Dense, Input, LeakyReLU, Conv2D, Flatten, Add, Cropping2D, Layer\n",
    "from tensorflow.keras.layers import UpSampling2D, Reshape, AveragePooling2D, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.activations import softplus as f\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import Sequence\n",
    "#from tensorflow.keras.datasets import mnist\n",
    "from keras.utils import Progbar\n",
    "\n",
    "from skimage.transform import resize\n",
    "from skimage import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an implementation of the StyleALAE which has a symmetrical encoder and decoder, using Instance Normalisation and Adaptive Instance Normalisation introduced [here](https://arxiv.org/pdf/1703.06868.pdf). IN is available as part of `tfa` and `AdaIn` is a custom layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- SET SEED --- #\n",
    "np.random.seed(0)\n",
    "# ---------------- #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal = tf.initializers.RandomNormal\n",
    "\n",
    "class DenseEQ(Dense):\n",
    "    \"\"\"\n",
    "    Standard dense layer but includes learning rate equilization\n",
    "    at runtime as per Karras et al. 2017.\n",
    "\n",
    "    Inherits Dense layer and overides the call method.\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        if 'kernel_initializer' in kwargs:\n",
    "            raise Exception(\"Cannot override kernel_initializer\")\n",
    "        super().__init__(kernel_initializer=normal(0, 1), **kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super().build(input_shape)\n",
    "        # The number of inputs\n",
    "        n = np.product([int(val) for val in input_shape[1:]])\n",
    "        # He initialisation constant\n",
    "        self.c = np.sqrt(2/n)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        output = K.dot(inputs, self.kernel*self.c) # scale kernel\n",
    "        if self.use_bias:\n",
    "            output = K.bias_add(output, self.bias, data_format='channels_last')\n",
    "        if self.activation is not None:\n",
    "            output = self.activation(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2DEQ(Conv2D):\n",
    "    \"\"\"\n",
    "    Standard Conv2D layer but includes learning rate equilization\n",
    "    at runtime as per Karras et al. 2017.\n",
    "\n",
    "    Inherits Conv2D layer and overrides the call method, following\n",
    "    https://github.com/keras-team/keras/blob/master/keras/layers/convolutional.py\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        if 'kernel_initializer' in kwargs:\n",
    "            raise Exception(\"Cannot override kernel_initializer\")\n",
    "        super().__init__(kernel_initializer=normal(0, 1), **kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super().build(input_shape)\n",
    "        # The number of inputs\n",
    "        n = np.product([int(val) for val in input_shape[1:]])\n",
    "        # He initialisation constant\n",
    "        self.c = np.sqrt(2/n)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        if self.rank == 2:\n",
    "            outputs = K.conv2d(\n",
    "                inputs,\n",
    "                self.kernel*self.c, # scale kernel\n",
    "                strides=self.strides,\n",
    "                padding=self.padding,\n",
    "                data_format=self.data_format,\n",
    "                dilation_rate=self.dilation_rate)\n",
    "\n",
    "        if self.use_bias:\n",
    "            outputs = K.bias_add(\n",
    "                outputs,\n",
    "                self.bias,\n",
    "                data_format=self.data_format)\n",
    "\n",
    "        if self.activation is not None:\n",
    "            return self.activation(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weighted sum output\n",
    "class WeightedSum(Add):\n",
    "    # init with default value\n",
    "    def __init__(self, alpha=0.0, **kwargs):\n",
    "        super(WeightedSum, self).__init__(**kwargs)\n",
    "        self.alpha = K.variable(alpha, name='ws_alpha')\n",
    "\n",
    "    # output a weighted sum of inputs\n",
    "    def _merge_function(self, inputs):\n",
    "        # only supports a weighted sum of two inputs\n",
    "        assert (len(inputs) == 2)\n",
    "        # ((1-a) * input1) + (a * input2)\n",
    "        output = ((1.0 - self.alpha) * inputs[0]) + (self.alpha * inputs[1])\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaInstanceNormalization(Layer):\n",
    "    \"\"\"\n",
    "    This is the AdaInstanceNormalization layer used by\n",
    "    \n",
    "    manicman199 available at https://github.com/manicman1999/StyleGAN-Keras\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "             axis=-1,\n",
    "             momentum=0.99,\n",
    "             epsilon=1e-3,\n",
    "             center=True,\n",
    "             scale=True,\n",
    "             **kwargs):\n",
    "        super(AdaInstanceNormalization, self).__init__(**kwargs)\n",
    "        self.axis = axis\n",
    "        self.momentum = momentum\n",
    "        self.epsilon = epsilon\n",
    "        self.center = center\n",
    "        self.scale = scale\n",
    "    \n",
    "    \n",
    "    def build(self, input_shape):\n",
    "    \n",
    "        dim = input_shape[0][self.axis]\n",
    "        if dim is None:\n",
    "            raise ValueError('Axis ' + str(self.axis) + ' of '\n",
    "                             'input tensor should have a defined dimension '\n",
    "                             'but the layer received an input with shape ' +\n",
    "                             str(input_shape[0]) + '.')\n",
    "    \n",
    "        super(AdaInstanceNormalization, self).build(input_shape) \n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "        input_shape = K.int_shape(inputs[0])\n",
    "        reduction_axes = list(range(0, len(input_shape)))\n",
    "        \n",
    "        beta = inputs[1]\n",
    "        gamma = inputs[2]\n",
    "\n",
    "        if self.axis is not None:\n",
    "            del reduction_axes[self.axis]\n",
    "\n",
    "        del reduction_axes[0]\n",
    "        mean = K.mean(inputs[0], reduction_axes, keepdims=True)\n",
    "        stddev = K.std(inputs[0], reduction_axes, keepdims=True) + self.epsilon\n",
    "        normed = (inputs[0] - mean) / stddev\n",
    "\n",
    "        return normed * gamma + beta\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'axis': self.axis,\n",
    "            'momentum': self.momentum,\n",
    "            'epsilon': self.epsilon,\n",
    "            'center': self.center,\n",
    "            'scale': self.scale\n",
    "        }\n",
    "        base_config = super(AdaInstanceNormalization, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanAndStDev(Layer):\n",
    "    \"\"\"\n",
    "    This is the Instance Normalization transformation which\n",
    "    concatentates mu and sigma to latter be mapped to w.\n",
    "    \"\"\"\n",
    "    def __init__(self,**kwargs):\n",
    "        super(MeanAndStDev, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(MeanAndStDev, self).build(input_shape) \n",
    "    \n",
    "    def call(self, inputs):\n",
    "        m = K.mean(inputs, axis=[1, 2], keepdims=True)\n",
    "        std = K.std(inputs, axis=[1, 2], keepdims=True)\n",
    "        statistics = K.concatenate([m, std], axis=1)\n",
    "        return statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the complexity of growing the encoder, it is better to create the encoder EncoderBlock as a Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(Model):\n",
    "    \"\"\"\n",
    "    Encoder block using instance normalisation to extract style.\n",
    "    \"\"\"\n",
    "    def __init__(self, filters, block, z_dim, **kwargs):\n",
    "        super(EncoderBlock, self).__init__(**kwargs)\n",
    "        # Attributes\n",
    "        self.filters = filters\n",
    "        self.block = block\n",
    "        self.z_dim = z_dim\n",
    "        dim =  2**(block+1)\n",
    "\n",
    "        # Trainable Layers\n",
    "        self.conv1 = Conv2DEQ(filters=filters, kernel_size=(3, 3), padding=\"same\", name=f\"E_block_{block}_Conv_1\")\n",
    "        self.act1 = LeakyReLU(0.2, name=f\"E_block_{block}_Act_1\")\n",
    "        self.msd = MeanAndStDev(name=f\"E_block_{block}_msd\")\n",
    "        self.in1 = InstanceNormalization(name=f\"E_block_{block}_IN_1\", center=False, scale=False)\n",
    "        self.in2 = InstanceNormalization(name=f\"E_block_{block}_IN_2\", center=False, scale=False)\n",
    "        self.conv2 = Conv2DEQ(filters=filters, kernel_size=(3, 3), padding=\"same\", name=f\"E_block_{block}_Conv_2\")\n",
    "        self.act2 = LeakyReLU(0.2, name=f\"E_block_{block}_Act_2\")\n",
    "        self.downsample = AveragePooling2D(name=f\"E_block_{block}_DownSample\")\n",
    "        self.mapStyle1 = DenseEQ(units=z_dim, name=f\"E_block_{block}_style_1\")\n",
    "        self.mapStyle2 = DenseEQ(units=z_dim, name=f\"E_block_{block}_style_2\")\n",
    "        self.flatten = Flatten(name=f\"E_block_{block}_flatten\")\n",
    "          \n",
    "    def call(self, inputs):\n",
    "        # Convolution 1\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.act1(x)\n",
    "        \n",
    "        # Instance Normalisation 1\n",
    "        style1 = self.flatten(self.msd(x))\n",
    "        x = self.in1(x)\n",
    "        \n",
    "        # Convolution 2\n",
    "        x = self.conv2(x)\n",
    "        if self.block > 1:\n",
    "            x = self.downsample(x)\n",
    "        x = self.act2(x)\n",
    "        \n",
    "        # Instance Normalisation 2\n",
    "        style2 = self.flatten(self.msd(x))\n",
    "        x = self.in2(x)\n",
    "        \n",
    "        # Affine transform to style vectors\n",
    "        w1 = self.mapStyle1(style1)\n",
    "        w2 = self.mapStyle2(style2)\n",
    "        \n",
    "        return x, w1, w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CropNoise(noise_tensor, size, block):\n",
    "    \"\"\"\n",
    "    Crops the noise_tensor to the target size.\n",
    "    \"\"\"\n",
    "    cut = (noise_tensor.shape[1]-size)//2\n",
    "    crop = Cropping2D(cut, name=f\"G_Noise_Crop_block_{block}\")(noise_tensor)\n",
    "    return crop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, building the GeneratorBlock as a Model makes progressive growing possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratorBlock(Model):\n",
    "    \"\"\"\n",
    "    Generator block using adaptive instance normalisation to inject style.\n",
    "    \"\"\"\n",
    "    def __init__(self, filters, block, **kwargs):\n",
    "        super(GeneratorBlock, self).__init__(**kwargs)\n",
    "        # Attributes\n",
    "        self.filters = filters\n",
    "        self.block = block\n",
    "        self.dim =  2**(block+1)\n",
    "\n",
    "        # Trainable Layers\n",
    "        \n",
    "        # phase 1\n",
    "        self.upsample =  UpSampling2D(name=f\"G_block_{block}_UpSample\")\n",
    "        self.beta1 = DenseEQ(units=filters, name=f\"G_block_{block}_beta1\")\n",
    "        self.beta1r = Reshape([1, 1, filters], name=f\"G_block_{block}_beta1_reshape\")\n",
    "        self.gamma1 = DenseEQ(units=filters, name=f\"G_block_{block}_gamma1\")\n",
    "        self.gamma1r = Reshape([1, 1, filters], name=f\"G_block_{block}_gamma1_reshape\")\n",
    "        self.noise1 = Conv2DEQ(filters=filters, kernel_size=1, padding='same', name=f\"G_block_{block}_noise_bias1\")\n",
    "        self.conv1 = Conv2DEQ(filters=filters, kernel_size=3, padding='same', name=f\"G_block_{block}_decoder_conv1\")\n",
    "        self.AdaIn1 = AdaInstanceNormalization(name=f\"G_block_{block}_AdaIN_1\")\n",
    "        self.addNoise1 = Add(name=f\"G_block_{block}_Add_1\")\n",
    "        self.act1 = LeakyReLU(0.2, name=f\"G_block_{block}_Act_1\")\n",
    "            \n",
    "        # phase 2\n",
    "        self.beta2 = DenseEQ(units=filters, name=f\"G_block_{block}_beta2\")\n",
    "        self.beta2r = Reshape([1, 1, filters], name=f\"G_block_{block}_beta2_reshape\")\n",
    "        self.gamma2 = Dense(units=filters, name=f\"G_block_{block}_gamma2\")\n",
    "        self.gamma2r = Reshape([1, 1, filters], name=f\"G_block_{block}_gamma2_reshape\")\n",
    "        self.noise2 = Conv2DEQ(filters=filters, kernel_size=1, padding='same', name=f\"G_block_{block}_noise_bias2\")\n",
    "        self.conv2 = Conv2DEQ(filters=filters, kernel_size=3, padding='same', name=f\"G_block_{block}_decoder_conv2\")\n",
    "        self.AdaIn2 = AdaInstanceNormalization(name=f\"G_block_{block}_AdaIN_2\")\n",
    "        self.addNoise2 = Add(name=f\"G_block_{block}_Add_2\")\n",
    "        self.act2 = LeakyReLU(0.2, name=f\"G_block_{block}_Act_2\")\n",
    "            \n",
    "    def call(self, inputs):\n",
    "        # Unpack inputs\n",
    "        input_tensor, noise_tensor, style_tensor = inputs\n",
    "        \n",
    "        # Get noise image for level\n",
    "        noise_tensor = CropNoise(noise_tensor, self.dim, self.block)\n",
    "        \n",
    "        if self.block > 1:\n",
    "            x = self.upsample(input_tensor)\n",
    "        else:\n",
    "            x = input_tensor\n",
    "        \n",
    "        # Phase 1\n",
    "        beta = self.beta1r(self.beta1(style_tensor))\n",
    "        gamma = self.gamma1r(self.gamma1(style_tensor))\n",
    "        noise = self.noise1(noise_tensor)\n",
    "        x = self.conv1(x)\n",
    "        x = self.AdaIn1([x, beta, gamma])\n",
    "        x = self.addNoise1([x, noise])\n",
    "        x = self.act1(x)\n",
    "        \n",
    "        # Phase 2\n",
    "        beta = self.beta2r(self.beta2(style_tensor))\n",
    "        gamma = self.gamma2r(self.gamma2(style_tensor))\n",
    "        noise = self.noise2(noise_tensor)\n",
    "        x = self.conv2(x)\n",
    "        x = self.AdaIn2([x, beta, gamma])\n",
    "        x = self.addNoise2([x, noise])\n",
    "        x = self.act2(x)\n",
    "        \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tRGB(x, block):\n",
    "    \"\"\"\n",
    "    A convolutional transformation to RGB space from filter space.\n",
    "    :param x: the tensor to transform\n",
    "    :param block: the block number\n",
    "    :return: rgb\n",
    "    \"\"\"\n",
    "    rgb = Conv2DEQ(filters=3, kernel_size=(1, 1), padding=\"same\", activation=\"sigmoid\", name=f\"G_block_{block}_tRGB\")(x)\n",
    "    return rgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM = 4 # the current size\n",
    "TARGET = 64 # the final size to grow to\n",
    "CHANNELS = 3 # the number of channels - 3 for rgb\n",
    "LEVELS = int(np.log2(DIM/2)) # the number of blocks 4-8-16 = 3 Levels\n",
    "X_DIM = (DIM, DIM, CHANNELS) # the shape of the current input image\n",
    "NOISE_DIM = (TARGET, TARGET, 1) # the shape of the noise image (always target size)\n",
    "Z_DIM = 128 # the dimension of the z vector\n",
    "BASE_CHANNELS = 128 # the number of filters at the base of the generator\n",
    "BATCH_SIZE = 16 # batchsize should be 16 for merged, and > for fine tuning if memory available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_input = Input(shape=(Z_DIM,))\n",
    "x = DenseEQ(units=Z_DIM)(F_input)\n",
    "x = LeakyReLU(0.2)(x)\n",
    "x = DenseEQ(units=Z_DIM)(x)\n",
    "x = LeakyReLU(0.2)(x)\n",
    "x = DenseEQ(units=Z_DIM)(x)\n",
    "x = LeakyReLU(0.2)(x)\n",
    "x = DenseEQ(units=Z_DIM)(x)\n",
    "F_output = LeakyReLU(0.2)(x)\n",
    "F = Model(inputs=[F_input],\n",
    "          outputs=[F_output], name=\"F\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_input = Input(shape=(Z_DIM,))\n",
    "x = DenseEQ(units=Z_DIM)(D_input)\n",
    "x = LeakyReLU(0.2)(x)\n",
    "x = DenseEQ(units=Z_DIM)(x)\n",
    "x = LeakyReLU(0.2)(x)\n",
    "D_output = DenseEQ(units=1)(x)\n",
    "D = Model(inputs=[D_input],\n",
    "          outputs=[D_output], name=\"D\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### G Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_inputs = [ Input(shape=(Z_DIM,), name=f\"G_w_input_{i+1}\") for i in range(LEVELS)]\n",
    "noise_input = Input(shape=NOISE_DIM, name=\"G_noise_input\")\n",
    "constant_input = Input(shape=(1, 1), name=\"G_constant_input\")\n",
    "\n",
    "\n",
    "G_inputs = w_inputs + [noise_input, constant_input]\n",
    "\n",
    "# Constant Start - 4x4x512\n",
    "x = Dense(units=4*4*BASE_CHANNELS, name=f\"G_base_{BASE_CHANNELS}\")(constant_input)\n",
    "x = Reshape((4, 4, BASE_CHANNELS), name=f\"G_base_reshape\")(x)\n",
    "\n",
    "block = 1\n",
    "style = G_inputs[block-1]\n",
    "x = GeneratorBlock(filters=BASE_CHANNELS, block=block, name=f\"G_block_{block}_style\")([x, noise_input, style])\n",
    "\n",
    "G_output = tRGB(x, block)\n",
    "\n",
    "G = Model(inputs=[G_inputs], outputs=[G_output], name=\"G\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#G.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_G(old_model, block, filters):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    assert(block > 1)\n",
    "    \n",
    "    # Create new inputs\n",
    "    w_inputs = [ Input(shape=(Z_DIM,), name=f\"G_w_input_{i+1}\") for i in range(block)]\n",
    "    noise_input = Input(shape=NOISE_DIM, name=\"G_noise_input\")\n",
    "    constant_input = Input(shape=(1, 1), name=\"G_constant_input\")\n",
    "    \n",
    "    # Pass through old model up to tRGB\n",
    "    noise = noise_input\n",
    "    constant = constant_input\n",
    "    x = old_model.get_layer(f\"G_base_{BASE_CHANNELS}\")(constant)\n",
    "    x = old_model.get_layer(\"G_base_reshape\")(x)\n",
    "    for b in range(block-1):\n",
    "        style = w_inputs[b]\n",
    "        x = old_model.get_layer(f\"G_block_{b+1}_style\")([x, noise, style ])\n",
    "    \n",
    "    # Get old RGB and upsample\n",
    "    old_out = old_model(w_inputs[:block-1] + [noise_input, constant])\n",
    "    old_out = UpSampling2D()(old_out)\n",
    "    \n",
    "    # Add new block\n",
    "    style = w_inputs[block-1]\n",
    "    x = GeneratorBlock(filters=filters, block=block, name=f\"G_block_{block}_style\")([x, noise, style])\n",
    "\n",
    "    # Transform to RGB\n",
    "    new_out = tRGB(x, block)\n",
    "    \n",
    "    # STRAIGHT MODEL\n",
    "    g_inputs = w_inputs + [noise_input, constant_input]\n",
    "    straight_g = Model(inputs=g_inputs, outputs=[new_out], name=f\"G_straight_{block}\")\n",
    "    \n",
    "    # MERGE MODEL\n",
    "    g_out = WeightedSum(name=\"Weight_Sum_G\")([old_out, new_out])\n",
    "    \n",
    "    merged_g = Model(inputs=g_inputs, outputs=[g_out], name=f\"G_merged_{block}\")\n",
    "    \n",
    "    return straight_g, merged_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#G_s, G_m = expand_G(old_model=G_s, block=3, filters=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlockRouter(Model):\n",
    "    def __init__(self, filters, block, z_dim, **kwargs):\n",
    "        \"\"\"\n",
    "        filters is a list that containers - [input_filters, block_filters]\n",
    "        \"\"\"\n",
    "        super(EncoderBlockRouter, self).__init__(**kwargs)\n",
    "        # Attributes\n",
    "        self.filters = filters\n",
    "        self.block = block\n",
    "        self.z_dim = z_dim\n",
    "        \n",
    "        # Parameters\n",
    "        self.conv1 = Conv2DEQ(filters=self.filters[0], kernel_size=(3, 3), padding=\"same\")\n",
    "        self.act1 = LeakyReLU(alpha=0.2)\n",
    "        self.encode = EncoderBlock(self.filters[1], self.block, self.z_dim, name=f\"E_block_{block}_encoder\")\n",
    "                            \n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.act1(x)\n",
    "            \n",
    "        x, w1, w2 = self.encode(x)\n",
    "        \n",
    "        return x, w1, w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM = 4\n",
    "E_input = Input(shape=(DIM, DIM, CHANNELS), name=\"E_input\")\n",
    "\n",
    "x, w1, w2 = EncoderBlockRouter(filters=[128, 128], block=1, z_dim=Z_DIM, name=\"encoder_block_router_1\")(E_input)\n",
    "\n",
    "w = Add(name=\"E_Final_Sum_base\")([w1, w2])\n",
    "\n",
    "E_output = w\n",
    "E = Model(inputs=[E_input], outputs=[E_output], name=\"E\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_E(old_model, filters, block):\n",
    "    \"\"\"\n",
    "    filters - [input_filters, block_filters]\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create new input\n",
    "    dim = int(old_model.get_input_at(0).shape[1]*2)\n",
    "    c = old_model.get_input_at(0).shape[-1]\n",
    "    e_in = Input(shape=(dim, dim, c), name=\"E_new_input\")\n",
    "\n",
    "    x_new, new_w1, new_w2 = EncoderBlockRouter(filters=filters, block=block,\n",
    "                                      z_dim=Z_DIM, name=f\"encoder_block_router_{block}\")(e_in)\n",
    "    \n",
    "    # Save values\n",
    "    x = x_new\n",
    "    ws = [new_w1, new_w2]\n",
    "    \n",
    "    # Pass through remaining layers of old model\n",
    "    for b in range(block-1, 0, -1):\n",
    "        if b == block-1 and b != 0:\n",
    "            x, w1, w2 = old_model.get_layer(f\"encoder_block_router_{b}\").encode(x)\n",
    "        else:\n",
    "            x, w1, w2 = old_model.get_layer(f\"E_block_{b}_encoder\")(x)\n",
    "        \n",
    "        ws.extend([w1, w2])\n",
    "    \n",
    "    e_out = Add(name=\"E_Final_Sum_Straight\")(ws)\n",
    "    \n",
    "    straight_e = Model(inputs=[e_in], outputs=[e_out], name=f\"E_straight_{dim}\")\n",
    "    \n",
    "    \n",
    "    # CREATE MERGE MODEL\n",
    "    \n",
    "    # Downsample input\n",
    "    e_old_in = AveragePooling2D(name=\"downsample\")(e_in)\n",
    "    \n",
    "    x = old_model.layers[1].layers[0](e_old_in)\n",
    "    x_old = old_model.layers[1].layers[1](x)\n",
    "    \n",
    "    # Merge into old model\n",
    "    x = WeightedSum(name=\"Weight_Sum_E\")([x_old, x_new])\n",
    "    \n",
    "    # Pass through remaining layers of old model\n",
    "    ws = [new_w1, new_w2]\n",
    "    for b in range(block-1, 0, -1):\n",
    "        if b == block-1 and b != 0:\n",
    "            x, w1, w2 = old_model.get_layer(f\"encoder_block_router_{b}\").encode(x)\n",
    "        else:\n",
    "            x, w1, w2 = old_model.get_layer(f\"E_block_{b}_encoder\")(x)\n",
    "            \n",
    "        ws.extend([w1, w2])\n",
    "        \n",
    "    e_out = Add(name=\"E_Final_Sum_Merge\")(ws)\n",
    "    \n",
    "    merged_e = Model(inputs=[e_in], outputs=[e_out], name=\"E_merged\")\n",
    "    \n",
    "    return straight_e, merged_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#E_s, E_m = expand_E(old_model=E_s, filters=[64, 64], block=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the sub-networks defined, we can put them together to create the Discriminator,Generator and Reciprocal networks, as well as the inference network.\n",
    "\n",
    "$$\n",
    "G = \\mathbb{G} \\circ F \\\\\n",
    "D = \\mathbb{D} \\circ E \\\\\n",
    "R = E \\circ \\mathbb{G} \\circ F \\\\\n",
    "I = E \\circ G\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the points for training\n",
    "#E = E_m\n",
    "#G = G_m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update at scale\n",
    "DIM = 4\n",
    "X_DIM = (DIM, DIM, CHANNELS)\n",
    "LEVELS = int(np.log2(DIM/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator_in = Input(shape=X_DIM)\n",
    "discriminator_out = D(E(discriminator_in))\n",
    "discriminator = Model(inputs=[discriminator_in],\n",
    "                     outputs=[discriminator_out],\n",
    "                     name=\"discriminator\"\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_input = Input(shape=(Z_DIM,))\n",
    "noise_input = Input(shape=NOISE_DIM)\n",
    "constant_input = Input(shape=(1, 1))\n",
    "\n",
    "generator_ins = [z_input, noise_input, constant_input]\n",
    "\n",
    "w = F(z_input)\n",
    "\n",
    "generator_out = G([w]*LEVELS + [noise_input, constant_input])\n",
    "generator = Model(inputs=generator_ins,\n",
    "                     outputs=[generator_out],\n",
    "                     name=\"generator\"\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reciprocal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_in = Input(shape=(Z_DIM,)) # W is input\n",
    "noise_input = Input(shape=NOISE_DIM)\n",
    "constant_input = Input(shape=(1, 1))\n",
    "\n",
    "reciprocal_ins = [w_in, noise_input, constant_input]\n",
    "\n",
    "g_ins = [w_in]*LEVELS + [noise_input, constant_input]\n",
    "reciprocal_out = E(G(g_ins))\n",
    "reciprocal = Model(inputs=reciprocal_ins,\n",
    "                     outputs=[reciprocal_out],\n",
    "                     name=\"reciprocal\"\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_in = Input(shape=X_DIM, name=\"inference_input\")\n",
    "noise_input = Input(shape=NOISE_DIM)\n",
    "constant_input = Input(shape=(1, 1))\n",
    "\n",
    "inference_ins = [inference_in, noise_input, constant_input]\n",
    "\n",
    "w = E(inference_in)\n",
    "g_ins = [w]*LEVELS + [noise_input, constant_input]\n",
    "inference_out = G(g_ins)\n",
    "inference = Model(inputs=inference_ins,\n",
    "                     outputs=[inference_out],\n",
    "                     name=\"inference\"\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(Sequence):\n",
    "    \"\"\"\n",
    "    A data generator that inherits the keras.utils.Sequence\n",
    "    class so that it can be used with multi-processing\n",
    "    \"\"\"\n",
    "    def __init__(self, directory, batch_size, img_dim=None,  shuffle=True,\n",
    "                 style=False, target_dim=None):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            directory - the directory where the images are\n",
    "            batch_size - the batch size\n",
    "            img_dim -  the size of the image if wanting to resize\n",
    "            shuffle - whether to shuffle the images\n",
    "            style - whether to include styleVAE noise as part of X\n",
    "            target_dim - the target size in progressive growing eg. 4->8->...256, target is 256\n",
    "        Ouputs:\n",
    "            X - batch of images (and styleVAE noise)\n",
    "            Y - batch of images\n",
    "        \"\"\"\n",
    "        self.dir = directory\n",
    "        self.batch_size = batch_size\n",
    "        self.img_dim = img_dim\n",
    "        self.shuffle = shuffle\n",
    "        self.style = style\n",
    "        self.target_dim = target_dim\n",
    "        # ---------------------------------------- #\n",
    "        self.files = np.array(os.listdir(self.dir))\n",
    "        self.n = len(self.files)\n",
    "        self.indices = np.arange(self.n)\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "        self.files_in_batch = []\n",
    "        self.style = style\n",
    "        self.pos = 0\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return self.n // self.batch_size\n",
    "\n",
    "    def _indices_at(self, batch_index):\n",
    "        \"\"\"Returns the list of indices for batch index\"\"\"\n",
    "        return self.indices[batch_index*self.batch_size:(batch_index+1)*self.batch_size]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indices of the batch\n",
    "        indices = self._indices_at(index)\n",
    "        \n",
    "        # Get list of files in batch\n",
    "        self.files_in_batch = self.files[indices]\n",
    "    \n",
    "        batch = []\n",
    "        self.files_in_batch = []\n",
    "        for i in indices:\n",
    "            \n",
    "            # Create filename \n",
    "            fname = os.path.join(self.dir, self.files[i])\n",
    "            \n",
    "            # Load image & scale between 0-1\n",
    "            img = io.imread(fname)[:, :, 0:3] / 255.\n",
    "                        \n",
    "            # resize\n",
    "            if img.shape[0] != self.img_dim:\n",
    "                img = resize(img, (self.img_dim, self.img_dim))\n",
    "            \n",
    "            batch.append(img)\n",
    "            \n",
    "        batch = np.stack(batch)\n",
    "        \n",
    "        if self.style:\n",
    "            \n",
    "            X = [\n",
    "                 # Input Image\n",
    "                 batch,\n",
    "                 # Noise Image\n",
    "                 np.random.normal(0, 1, (self.batch_size, self.target_dim, self.target_dim, 1)),\n",
    "                \n",
    "                 # Constant input\n",
    "                 np.ones((self.batch_size, 1, 1)) \n",
    "                ]\n",
    "            return X\n",
    "        \n",
    "        # Standard\n",
    "        else:\n",
    "            return batch, batch\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(self.n)\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def set_files(self, files):\n",
    "            self.files = files\n",
    "            self.n = len(self.files)\n",
    "            self.shuffle = True\n",
    "            self.indices = np.arange(self.n)\n",
    "            print(\"setting files and resetting generator.\")\n",
    "            \n",
    "    def __next__(self):\n",
    "        if self.pos >= (self.n // self.batch_size):\n",
    "            self.pos = -1\n",
    "        result = self[self.pos]\n",
    "        self.pos += 1\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"/media/simon/2TB - Backup/Datasets/anime-faces/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gen = DataGenerator(directory=DATA_DIR, batch_size=BATCH_SIZE,\n",
    "                         img_dim=DIM, style=True, target_dim=TARGET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = data_gen[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check Network Functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_DIR = \"./img_out/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False, False, False, ..., False, False, False],\n",
       "       [False, False, False, ..., False, False, False],\n",
       "       [False, False, False, ..., False, False, False],\n",
       "       ...,\n",
       "       [False, False, False, ..., False, False, False],\n",
       "       [False, False, False, ..., False, False, False],\n",
       "       [False, False, False, ..., False, False, False]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.normal(0, 1, (32, Z_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_test = np.random.normal(0, 1, (32, Z_DIM))\n",
    "data_gen.batch_size = 32\n",
    "test_batch = data_gen[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_progress(z_test, test_batch, epoch, save=False, n=16):\n",
    "    \n",
    "    # Generate Random Samples\n",
    "    samples = generator.predict([z_test[:n]] + [test_batch[1][:n]] + [test_batch[2][:n]] )\n",
    "    # Reconstruction Inputs\n",
    "    recons = inference.predict(test_batch)\n",
    "    \n",
    "    # Show progress\n",
    "    fig, ax = plt.subplots(3, n, figsize=(n, 3))\n",
    "    for i in range(n):\n",
    "        ax[0,i].imshow(batch[0][i])\n",
    "        ax[0,i].axis(\"off\")\n",
    "        ax[1,i].imshow(recons[i])\n",
    "        ax[1,i].axis(\"off\")\n",
    "        ax[2,i].imshow(samples[i])\n",
    "        ax[2,i].axis(\"off\")\n",
    "\n",
    "\n",
    "    plt.text(-0.8, 0.4, \"Orig.\", transform=ax[0,0].transAxes)\n",
    "    plt.text(-1, 0.4, \"Recon.\", transform=ax[1,0].transAxes)\n",
    "    plt.text(-1.1, 0.4, \"Sample\", transform=ax[2,0].transAxes)\n",
    "    \n",
    "    plt.text(0, 1.2, f\"{epoch:04d}\", transform=ax[0,0].transAxes)\n",
    "\n",
    "    if save:\n",
    "        dim = test_batch[0].shape[1]\n",
    "        fname = os.path.join(OUT_DIR, f\"progress_{dim:04d}_{epoch:04d}.png\")\n",
    "        plt.savefig(fname, )\n",
    "        print(\"Plot saved at\", fname)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x216 with 48 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualise_progress(z_test, test_batch, 2, save=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2(x_true, x_pred):\n",
    "    \"\"\"\n",
    "    L2 for the reciprocal loss in latent space\n",
    "    \n",
    "    :param - x_true - groundtruth\n",
    "    :param - x_pred - prediction\n",
    "    \"\"\"\n",
    "    return K.mean((x_true - x_pred)**2)\n",
    "\n",
    "def discriminator_logistic_non_saturating(d_real, d_fake): \n",
    "    \"\"\"\n",
    "    Discriminator loss, where f = softplus.\n",
    "    \n",
    "    :param - d_real - discriminator real output\n",
    "    :param - d_fake - discriminator real output\n",
    "    \"\"\"\n",
    "    loss = f(-d_real) + f(d_fake)\n",
    "  \n",
    "    return K.mean(loss)\n",
    "\n",
    "\n",
    "def generator_logistic_non_saturating(g_result):\n",
    "    \"\"\"\n",
    "    generator loss, where f = softplus\n",
    "    \"\"\"\n",
    "    loss = f(-g_result)\n",
    "    return K.mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "_F = F.trainable_weights\n",
    "_G = G.trainable_weights\n",
    "_E = E.trainable_weights\n",
    "_D = D.trainable_weights\n",
    "\n",
    "# Hyper-parameters\n",
    " = 0.001\n",
    "1 = 0.0\n",
    "2 = 0.99\n",
    " = K.epsilon()\n",
    "=0.1\n",
    "\n",
    "# Optimizers\n",
    "Adam_D = Adam(, 1, 2, )\n",
    "Adam_G = Adam(, 1, 2, )\n",
    "Adam_R = Adam(, 1, 2, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      " 379/1346 [=======>......................] - ETA: 3:30 - Loss_D: 1.4320 - Loss_G: 0.7239 - Loss_R: 0.4678 - R1: 1.4279"
     ]
    }
   ],
   "source": [
    "epochs = 23 # 23 epochs = 500,000 training samples\n",
    "BATCH_SIZE = 16 # 16 during merge\n",
    "\n",
    "# Update data generator\n",
    "data_gen.img_dim = DIM\n",
    "data_gen.batch_size = BATCH_SIZE\n",
    "test_batch = data_gen[0]\n",
    "\n",
    "steps_per_epoch = data_gen.n // data_gen.batch_size\n",
    "\n",
    "# Alphas for transition\n",
    "step_size = 1/(steps_per_epoch*epochs)\n",
    "alphas = np.arange(0, 1+step_size, step_size)\n",
    "step = 1\n",
    "merge = False\n",
    "\n",
    "\n",
    "losses = {  \"Loss_D\" : [],\n",
    "            \"Loss_G\" : [],\n",
    "            \"Loss_R\" : [],\n",
    "         }\n",
    "\n",
    "\n",
    "# START\n",
    "for epoch in range(0, epochs):\n",
    "    \n",
    "    print(\"Epoch:\", epoch+1)\n",
    "    \n",
    "    # Create progress bar for current epoch\n",
    "    progress = Progbar(steps_per_epoch, width=30, verbose=1, interval=1)\n",
    "\n",
    "    for i in range(steps_per_epoch):\n",
    "        \n",
    "        if merge:\n",
    "            # Set WeigtedSum Alphas\n",
    "            K.set_value(G.get_layer(\"Weight_Sum_G\").alpha, alphas[step])\n",
    "            K.set_value(E.get_layer(\"Weight_Sum_E\").alpha, alphas[step])\n",
    "            step += 1\n",
    "        \n",
    "\n",
    "        # -------------------------------#\n",
    "        # Step I - Update Discriminator  #\n",
    "        # -------------------------------#\n",
    "        \n",
    "        # Random mini-batch from dataset\n",
    "        batch = data_gen[i]\n",
    "        \n",
    "        x_real = batch[0]\n",
    "        noise = batch[1]\n",
    "        constant = batch[2]\n",
    "\n",
    "        # samples from prior N(0, 1)\n",
    "        z = np.random.normal(0, 1, (BATCH_SIZE, Z_DIM))\n",
    "        # generate fake images\n",
    "        x_fake = generator.predict([z, noise, constant])\n",
    "\n",
    "        # Compute loss and apply gradients\n",
    "        with tf.GradientTape() as tape:\n",
    "            \n",
    "            fake_pred = discriminator(x_fake)\n",
    "            \n",
    "            real_pred = discriminator(x_real)\n",
    "            \n",
    "            loss_d = discriminator_logistic_non_saturating(real_pred, fake_pred)\n",
    "        \n",
    "            #Add the R1 term\n",
    "            if  > 0:\n",
    "                x_real = tf.Variable(x_real, dtype=tf.float32)\n",
    "                with tf.GradientTape() as r1_tape:\n",
    "                    r1_tape.watch(x_real)\n",
    "                    # 1. Get the discriminator output for real images\n",
    "                    pred = discriminator(x_real)\n",
    "\n",
    "                # 2. Calculate the gradients w.r.t to the real images.\n",
    "                grads = r1_tape.gradient(pred, [x_real])[0]\n",
    "\n",
    "                # 3. Calcuate the squared norm of the gradients\n",
    "                r1_penalty = K.sum(K.square(grads))\n",
    "                loss_d += /2 * r1_penalty\n",
    "            \n",
    "            \n",
    "        gradients = tape.gradient(loss_d, _E+_D)\n",
    "        Adam_D.apply_gradients(zip(gradients, _E+_D))\n",
    "        \n",
    "        # ----------------------------#\n",
    "        #  Step II - Update Generator #\n",
    "        # ----------------------------#\n",
    "        \n",
    "        # samples from prior N(0, 1)\n",
    "        z = np.random.normal(0, 1, (BATCH_SIZE, Z_DIM))\n",
    "        # Compute loss and apply gradients\n",
    "        with tf.GradientTape() as tape:\n",
    "\n",
    "            fake_pred = discriminator(generator([z, noise, constant]))\n",
    "\n",
    "            loss_g = generator_logistic_non_saturating(fake_pred)\n",
    "\n",
    "        gradients = tape.gradient(loss_g, _F+_G)\n",
    "        Adam_G.apply_gradients(zip(gradients, _F+_G))\n",
    "        \n",
    "        # ------------------------------#\n",
    "        #  Step III - Update Reciprocal #\n",
    "        # ------------------------------#\n",
    "        \n",
    "        # samples from prior N(0, 1)\n",
    "        z = np.random.normal(0, 1, (BATCH_SIZE, Z_DIM))\n",
    "        # Get w\n",
    "        w = F(z)\n",
    "        # Compute loss and apply gradients\n",
    "        with tf.GradientTape() as tape:\n",
    "            \n",
    "            w_pred = reciprocal([w, noise, constant])\n",
    "            \n",
    "            loss_r = l2(w, w_pred)\n",
    "            \n",
    "        gradients = tape.gradient(loss_r, _G+_E)\n",
    "        Adam_R.apply_gradients(zip(gradients, _G+_E))\n",
    "        \n",
    "        \n",
    "        if i % 10 == 9:\n",
    "            \n",
    "            loss_d = loss_d.numpy()\n",
    "            loss_g = loss_g.numpy()\n",
    "            loss_r = loss_r.numpy()\n",
    "            \n",
    "            # Progress\n",
    "            progress.update(i, values=[ (\"Loss_D\", loss_d),\n",
    "                                        (\"Loss_G\", loss_g),\n",
    "                                        (\"Loss_R\", loss_r),\n",
    "                                        (\"R1\", r1_penalty.numpy())\n",
    "                                      ])\n",
    "\n",
    "            # Save losses each step\n",
    "            losses[\"Loss_D\"].append(loss_d)\n",
    "            losses[\"Loss_G\"].append(loss_g)\n",
    "            losses[\"Loss_R\"].append(loss_r)\n",
    "    \n",
    "    # ---------------#\n",
    "    #  END OF EPOCH  #\n",
    "    # ---------------#\n",
    "    \n",
    "    # LOSSES\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    ax[0].plot(range(len(losses[\"Loss_D\"])), losses[\"Loss_D\"], label=\"Discriminator\")\n",
    "    ax[0].plot(range(len(losses[\"Loss_G\"])), losses[\"Loss_G\"], label=\"Generator\")\n",
    "    ax[0].set_xlabel(\"Training Steps In Epoch\")\n",
    "    ax[0].set_ylabel(\"Adversarial Loss\")\n",
    "    ax[0].legend()\n",
    "\n",
    "    ax[1].plot(range(len(losses[\"Loss_R\"])), losses[\"Loss_R\"])\n",
    "    ax[1].set_xlabel(\"Training Steps In Epoch\")\n",
    "    ax[1].set_ylabel(\"Reciprocity Loss\")\n",
    "    plt.show()\n",
    "    \n",
    "    # IMAGES\n",
    "    visualise_progress(z_test, test_batch, epoch, save=True)\n",
    "    \n",
    "    print(\"---------------------------------------------------------\")\n",
    "    \n",
    "    \n",
    "# Save weights and losses\n",
    "G.save_weights(f\"./weights/StyleWeights/G_{DIM}x{DIM}_weights.h5\")\n",
    "E.save_weights(f\"./weights/StyleWeights/E_{DIM}x{DIM}_weights.h5\")\n",
    "F.save_weights(f\"./weights/StyleWeights/F_{DIM}x{DIM}_weights.h5\")\n",
    "D.save_weights(f\"./weights/StyleWeights/D_{DIM}x{DIM}_weights.h5\")\n",
    "\n",
    "pickle_out = open(f\"./losses/losses_{DIM}x{DIM}.pickle\",\"wb\")\n",
    "pickle.dump(losses, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
